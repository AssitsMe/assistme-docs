---
description: AssistMe tracks token usage for each generation for cost and usage reporting. If tokens are not provided in the generation, AssistMe will automatically calculate the token usage.
---

# Token usage

Token usage numbers are used across the AssistMe interface and reports.

```typescript
generation = {
  ...
  usage: {
    prompt: number,
    completion: number,
    total: number,
    unit: string, // 'TOKENS' or 'CHARACTERS', defaults to 'TOKENS'
  },
  ...
}
```

## Ingestion of usage

When ingesting LLM generations into AssistMe you can add token usage numbers to the generation object. If available in the LLM response, this is the preferred way to track usage with AssistMe.

## Built-in token calculation

For ingested generations without usage attributes, AssistMe automatically calculates token amounts. The correct tokenizer is selected based on the `model` attribute of the generation.

This is helpful for LLM-APIs that do not return usage information in the response, e.g., when streaming OpenAI completions.

| Model           | Tokenizer     | Package                                                                            |
| --------------- | ------------- | ---------------------------------------------------------------------------------- |
| `gpt*`          | `cl100k_base` | [`tiktoken`](https://www.npmjs.com/package/tiktoken)                               |
| `text-davinci*` | `p50k_base`   | [`tiktoken`](https://www.npmjs.com/package/tiktoken)                               |
| `claude*`       | `claude`      | [`@anthropic-ai/tokenizer`](https://www.npmjs.com/package/@anthropic-ai/tokenizer) |

Need another tokenizer? Create an issue on [GitHub](/issue).

## Compatibility with OpenAI

AssistMe is compatible with the usage objects returned by the OpenAI API. The AssistMe SDKs automatically convert the OpenAI usage object to the more generic AssistMe usage object.

```typescript
usage = {
  promptTokens: 50,
  completionTokens: 49,
  totalTokens: 99,
};
```
