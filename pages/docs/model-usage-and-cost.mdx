---
description: >-
  AssistMe tracks usage and cost of LLM generations for various models (incl OpenAI, Anthropic, Google, and more). Add your own model definitions to track any model or custom pricing.
---

# Model Usage & Cost

Across AssistMe, usage and cost are tracked for LLM generations:

- **Usage**: token/character counts
- **Cost**: USD cost of the generation

Both usage and cost can be either

- [**ingested**](#ingest) via API, SDKs or integrations
- or [**inferred**](#infer) based on the `model` parameter of the generation. AssistMe comes with a list of predefined popular models and their tokenizers including OpenAI, Anthropic, and Google models.

Ingested usage and cost are prioritized over inferred usage and cost:

```mermaid
flowchart LR
  A[Ingested Observation]
  B["Usage (tokens or other unit)"]
  C["Cost (in USD)"]
  A --> D{Includes usage?}
  D -->|Yes| B
  D -->|No| E(Use tokenizer) --> B
  A --> F{Includes cost?}
  F -->|Yes| C
  F -->|No| G(Use model price/unit) --> C
  B -->|use usage| G
```

## Ingest usage and/or cost [#ingest]

If available in the LLM response, ingesting usage and/or cost is the most accurate and robust way to track usage in AssistMe:

```typescript
langfuse.generation({
  ...
  usage: {
    // usage
    input: integer,
    output: integer,
    total: integer, // if no separate input&output, otherwise derived from input + output
    unit: string, // 'TOKENS' or 'CHARACTERS', defaults to 'TOKENS'

    // usd cost
    inputCost: number
    outputCost: number
    totalCost: number
  },
  ...
})
```

### Compatibility with OpenAI

For increased compatibility with OpenAI, you can also use the following attributes to ingest usage:

```typescript
langfuse.generation({
  usage: {
    // usage
    promptTokens: integer,
    completionTokens: integer,
    totalTokens: integer, // optional, derived from prompt + completion
  },
});
```

## Infer usage and/or cost [#infer]

If either usage or cost are not ingested, AssistMe will attempt to infer the missing values based on the `model` parameter of the generation. This is especially useful for some model providers or self-hosted models which do not include usage or cost in the response.

AssistMe comes with a **list of predefined popular models and their tokenizers** including **OpenAI, Anthropic, Google**.

You can also add your own **custom model definitions** (see [below](#custom-model-definitions)) or request official support for new models via [GitHub](/issue).

### Usage

If a tokenizer is specified for the model, AssistMe automatically calculates token amounts for ingested generations.

The following tokenizers are currently supported:

| Tokenizer | Package                                                                            |
| --------- | ---------------------------------------------------------------------------------- |
| `openai`  | [`js-tiktoken`](https://www.npmjs.com/package/js-tiktoken)                         |
| `claude`  | [`@anthropic-ai/tokenizer`](https://www.npmjs.com/package/@anthropic-ai/tokenizer) |

### Cost

Model definitions include prices per unit (input, output, total).

AssistMe automatically calculates cost for ingested generations if (1) usage is ingested or inferred, (2) and a matching model definition includes prices.

## Troubleshooting

**Usage and cost are missing for historical generations**. Except for changes in prices, AssistMe does not retroactively infer usage and cost for existing generations when model definitions are changed. You can request a batch job (AssistMe Cloud) or run a [script](/docs/deployment/self-host#migrate-models) (self-hosting) to apply new model definitions to existing generations.
