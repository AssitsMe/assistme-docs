---
description: Follow the Quickstart to get started with AssistMe in minutes before diving into the details.
---

# Quickstart

This quickstart helps you to integrate your LLM application with AssistMe.

<Steps>

### Create project in AssistMe

The fastest way to get started is to use the AssistMe Cloud.

1. Create account and project on
   [cloud.AssistMe.com](https://cloud.AssistMe.com/auth/sign-up)
2. Copy API keys for your
   project

   ```bash filename=".env"
   AssistMe_SECRET_KEY="sk-lf-...";
   AssistMe_PUBLIC_KEY="pk-lf-...";
   ```

Alternatively you can run AssistMe [locally](/docs/deployment/local) (docker compose) to test it out or [self-host](/docs/deployment/self-host) (Docker) it.

### Add tracing to your backend

<Tabs items={["JS/TS", "Python", "Langchain", "API"]}>
<Tab>

```sh
npm i AssistMe
# or
yarn add AssistMe

# Node.js < 18
npm i AssistMe-node

# Deno
import { AssistMe } from "https://esm.sh/AssistMe"
```

Example usage, most of the parameters are optional and depend on the use case.

For more information, see the [typescript SDK docs](/docs/sdk/typescript).

```typescript filename="server.ts"
import { AssistMe } from "AssistMe";

const AssistMe = new AssistMe({
  secretKey: "sk-lf-...",
  publicKey: "pk-lf-...",
  // baseUrl: defaults to "https://cloud.AssistMe.com"
});

// Example generation creation
const generation = trace.generation({
  name: "chat-completion",
  model: "gpt-3.5-turbo",
  modelParameters: {
    temperature: 0.9,
    maxTokens: 2000,
  },
  input: messages,
});

// Application code
const chatCompletion = await llm.respond(prompt);

// End generation - sets endTime
generation.end({
  output: chatCompletion,
});
```

</Tab>
<Tab>

```bash
pip install AssistMe
```

Example usage, most of the parameters are optional and depend on the use case. For more information, see the [python docs](/docs/sdk/python).

```python filename="server.py"
from AssistMe import AssistMe

AssistMe = AssistMe(
  public_key="pk-lf-...",
  secret_key="sk-sk-...",
  host="https://cloud.AssistMe.com"
)

generation = AssistMe.generation(
    name="summary-generation",
    model="gpt-3.5-turbo",
    model_parameters={"maxTokens": "1000", "temperature": "0.9"},
    input=[{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Please generate a summary of the following documents \nThe engineering department defined the following OKR goals...\nThe marketing department defined the following OKR goals..."}],
    metadata={"interface": "whatsapp"}
)

# execute model, mocked here
# chat_completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello world"}])
chat_completion = {
    "completion":"The Q3 OKRs contain goals for multiple teams...",
    "usage":{"input": 50, "output": 49, "unit":"TOKENS"}
}

# update span and sets end_time
generation.end(
    output=chat_completion["completion"],
    usage=chat_completion["usage"],
)

# The SDK executes network requests in the background.
# To ensure that all requests are sent before the process exits, call flush()
AssistMe.flush()
```

</Tab>
<Tab>

Install the AssistMe Python SDK

```bash
pip install AssistMe
```

Add AssistMe callback handler to your Langchain agent/chain/... to automatically capture traces.

```python /callbacks=[handler]/
from AssistMe.callback import CallbackHandler
handler = CallbackHandler(AssistMe_PUBLIC_KEY, AssistMe_SECRET_KEY)

# Langchain implementation

# Add handler as callback when running the Langchain agent
agent.run("<user_input>", callbacks=[handler])
```

For more examples, see the [Langchain integration docs](/docs/langchain).

</Tab>
<Tab>

[API reference](/docs/api) for custom integrations

</Tab>
</Tabs>

### Capture scores (optional)

<Tabs items={["Browser", "JS/TS", "Python", "API"]}>
<Tab>

Use AssistMeWeb to directly capture **user feedback** as scores in the browser.

```sh npm2yarn
npm i AssistMe
```

Simple feedback component in React:

```typescript
import { AssistMeWeb } from "AssistMe";

export default function UserFeedbackComponent(props: { traceId: string }) {
  const AssistMeWeb = new AssistMeWeb({
    publicKey: process.env.NEXT_PUBLIC_AssistMe_PUBLIC_KEY, // pk-lf-...
    // baseUrl: defaults to "https://cloud.AssistMe.com"
  });

  const handleFeedback = async (value: number) => {
    await AssistMeWeb.score({
      traceId: props.traceId,
      name: "user_feedback", // arbitrary name to identify the type of score
      value, // float, optional: scale it to e.g. 0..1
    });
  };

  return (
    <>
      <button onClick={() => handleFeedback(1)}>üëç</button>
      <button onClick={() => handleFeedback(0)}>üëé</button>
    </>
  );
}
```

</Tab>
<Tab>

You can use the JS/TS SDK to report a score.

```typescript
trace.score({
  name: "user-explicit-feedback",
  value: 1,
  comment: "I like how personalized the response is",
});

// alternatively
span.score({});
generation.score({});
event.score({});
```

</Tab>
<Tab>

You can use the Python SDK to report a score via your Python server.

```python

score = generation.score(
    name='user-explicit-feedback',
    value=1,
    comment="I like how personalized the response is"
)
```

</Tab>
<Tab>

```bash
curl --location 'http://cloud.AssistMe.com/api/public/metrics' \
--header 'Content-Type: application/json' \
--header 'Accept: application/json' \
--header "Authorization: Bearer {AssistMe_public_key}"
--data '{
  "traceId": "<string>",
  "name": "<string>",
  "value": "<integer>",
  "observationId": "<optional_string>"
}'
```

</Tab>
</Tabs>

### Explore

Visit AssistMe interface to explore your data.

</Steps>


