# Overview

An agent is a configured instance of artificial intelligence capable of searching for documents, executing tasks using external tools, and **interacting with users via text or voice**. After the search phase, responses are generated by large language models such as **ChatGPT, Mistral, or LLaMA**. Certain AI agents, specifically designed to assist individuals with their daily tasks, are known as **'Co-pilots'**.

## How it works (high-level)

<Frame border fullWidth>
  ![Agent overview](/images/agent-overview-diagram.png)
</Frame>

## Large Language Models (LLMs)

Agents utilize large language models. The choice of the language model is a critical first decision. A prompt is a piece of natural language text that describes the desired behavior of the assistant. If crafted effectively, it can influence the tone and type of response from the model, such as making it more empathetic or less so, adjusting the length of the response, and dictating how to reply when information about a question is missing. Language models are highly sensitive to the system prompt, and even minor changes can lead to significant differences in the quality of answers.

## Prompt engineering

The rules for prompting may vary across different language models, necessitating adjustments to find the optimal prompt for each one. It is advisable to use evaluation methods after making prompt changes to assess their impact.

Prompts can include additional variables to customize the agent's behavior based on extra information about the user or other factors. Variables are enclosed in curly brackets in the format: `{variable_name}`. For instance, you might use the prompt: 'Adjust your response style and word choice based on the user's age. The user's age is: `{user_age}`.' The Varex API can be utilized to pass these variables during the call.


### Example using API or Python client:


<Tabs items={["API", "Python"]}>
  <Tab>
 
  ```bash
  curl --location 'http://cloud.varex.ai/api/v1/completions' \
  --header 'Content-Type: application/json' \
  --header 'Accept: application/json' \
  --header "Authorization: Bearer {Varex_public_key}"
  --data '{
    "agent": "5ca742a8-4481-4c6d-9b3c-b6d51b009cb9",
    "messages": "[
            {"role": "user", "content": "Hey, why did I get charged a fee for my checking account this month?"},
            {"role": "assistant", "content": "I can help with that. Usually, fees are for things like going below a minimum balance or using another bank's ATM. Did any of these happen recently?"},
            {"role": "user", "content": "Oh, I did use another ATM last week."},
            {"role": "assistant", "content": "That's likely the reason. Using an ATM from another bank can have a fee. To avoid this, try to use only ATMs from our bank. Anything else you need help with?"}
        ]",
    "variables": "{"user_age": 30} "
  }'
  ```
 
  </Tab>

    <Tab>

```python
from assistme import AssistMe
client = AssistMe()

response = client.chat.completions.create(
  agent="5ca742a8-4481-4c6d-9b3c-b6d51b009cb9",  # Specify the unique agent identifier
  messages=[
    {"role": "user", "content": "Hey, why did I get charged a fee for my checking account this month?"},
    {"role": "assistant", "content": "I can help with that. Usually, fees are for things like going below a minimum balance or using another bank's ATM. Did any of these happen recently?"},
    {"role": "user", "content": "Oh, I did use another ATM last week."},
    {"role": "assistant", "content": "That's likely the reason. Using an ATM from another bank can have a fee. To avoid this, try to use only ATMs from our bank. Anything else you need help with?"}
  ],
  variables={"user_age": 30}  # Including user age to adapt the response style
)
```
  </Tab>

  </Tabs>
